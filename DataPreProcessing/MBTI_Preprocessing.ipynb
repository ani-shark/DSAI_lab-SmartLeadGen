{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e286d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import kagglehub\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c85771",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kaggle_path = kagglehub.dataset_download(\"datasnaek/mbti-type\")\n",
    "    print(\"Kaggle dataset downloaded to:\", kaggle_path)\n",
    "    mbti_csv_path = os.path.join(kaggle_path, \"mbti_1.csv\")\n",
    "    if os.path.exists(mbti_csv_path):\n",
    "        print(\"Dataset successfully found at:\", mbti_csv_path)\n",
    "    else:\n",
    "        print(\"Dataset not found in the expected location.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occured while downloadin the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d01da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hf_dataset = load_dataset(\"jingjietan/pandora-big5\")\n",
    "    print(\"Hugging Face dataset loaded successfully.\")\n",
    "    print(\"Dataset structure:\", hf_dataset)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occured while loading the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6286bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_mbti = pd.read_csv(mbti_csv_path)\n",
    "    print(\"Kaggle CSV loaded into DataFrame:\")\n",
    "    df_mbti.info()\n",
    "except NameError:\n",
    "    print(\"Variable 'mbti_csv_path' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_big5 = hf_dataset['train'].to_pandas()\n",
    "    print(\"Hugging Face dataset converted to DataFrame:\")\n",
    "    df_big5.info()\n",
    "except NameError:\n",
    "    print(\"Variable 'hf_dataset' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)\n",
    "nltk.download('omw-1.4', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatier = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags = re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    cleaned_tokens = [lemmatier.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676362c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbti['I-E'] = df_mbti['type'].apply(lambda x: x[0])\n",
    "df_mbti['N-S'] = df_mbti['type'].apply(lambda x: x[1])\n",
    "df_mbti['T-F'] = df_mbti['type'].apply(lambda x: x[2])\n",
    "df_mbti['J-P'] = df_mbti['type'].apply(lambda x: x[3])\n",
    "\n",
    "expanded_rows = []\n",
    "for index, row in df_mbti.iterrows():\n",
    "    posts = row['posts'].split('|||')\n",
    "    for post in posts:\n",
    "        cleanded_post = clean_text(post)\n",
    "        if len(cleanded_post.strip()) > 0:\n",
    "            new_row = {\n",
    "                'text': cleanded_post,\n",
    "                'type': row['type'],\n",
    "                'I-E': row['I-E'], 'N-S': row['N-S'], 'T-F': row['T-F'], 'J-P': row['J-P']\n",
    "            }\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "df_mbti_processed = pd.DataFrame(expanded_rows)\n",
    "print(f\"Original Kaggle dataset: {len(df_mbti)} rows\")\n",
    "print(f\"Processed Kaggle dataset: {len(df_mbti_processed)} rows\\n\")\n",
    "print(df_mbti_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43367f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_dir = \"EDA_outputs\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Distribution of MBTI Dimensions')\n",
    "\n",
    "sns.countplot(ax=axes[0, 0], x='I-E', data=df_mbti_processed, order=['I', 'E'])\n",
    "axes[0, 0].set_title('Introversion vs. Extroversion')\n",
    "\n",
    "sns.countplot(ax=axes[0, 1], x='N-S', data=df_mbti_processed, order=['N', 'S'])\n",
    "axes[0, 1].set_title('Intuition vs. Sensing')\n",
    "\n",
    "sns.countplot(ax=axes[1, 0], x='T-F', data=df_mbti_processed, order=['T', 'F'])\n",
    "axes[1, 0].set_title('Thinking vs. Feeling')\n",
    "\n",
    "sns.countplot(ax=axes[1, 1], x='J-P', data=df_mbti_processed, order=['J', 'P'])\n",
    "axes[1, 1].set_title('Judging vs. Perceiving')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "distribution_mbti_plot_path = os.path.join(output_dir, 'distribution_mbti.png')\n",
    "plt.savefig(distribution_mbti_plot_path)\n",
    "print(f\"Saved diagnostic plot to '{distribution_mbti_plot_path}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_big5_to_mbti(row):\n",
    "    \n",
    "    i_e = 'E' if row['E'] > 50 else 'I'\n",
    "    n_s = 'N' if row['O'] > 50 else 'S'    \n",
    "    t_f = 'T' if row['A'] < 50 else 'F' \n",
    "    j_p = 'J' if row['C'] > 50 else 'P'\n",
    "    return i_e, n_s, t_f, j_p\n",
    "\n",
    "mbti_labels = df_big5.apply(map_big5_to_mbti, axis=1, result_type='expand')\n",
    "df_big5[['I-E', 'N-S', 'T-F', 'J-P']] = mbti_labels\n",
    "df_big5['type'] = df_big5['I-E'] + df_big5['N-S'] + df_big5['T-F'] + df_big5['J-P']\n",
    "\n",
    "df_big5['text'] = df_big5['text'].apply(clean_text)\n",
    "df_big5_processed = df_big5\n",
    "\n",
    "print(\"Big Five dataset processed and mapped to MBTI labels.\")\n",
    "print(df_big5_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = ['text', 'type', 'I-E', 'N-S', 'T-F', 'J-P']\n",
    "\n",
    "df_final = pd.concat(\n",
    "    [df_mbti_processed[common_columns], df_big5_processed[common_columns]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "output_filename = 'preprocessed_personality_data.csv'\n",
    "df_final.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Preprocessing complete! Combined dataset saved to '{output_filename}'\")\n",
    "print(f\"Total rows in final dataset: {len(df_final)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
