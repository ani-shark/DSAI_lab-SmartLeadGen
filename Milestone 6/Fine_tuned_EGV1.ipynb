{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b0e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ced8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "OUTPUT_DIR = \"mistral-7b-enron-email-finetune\"\n",
    "SAMPLES_TO_USE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt defines the output style and structure the model should learn.\n",
    "SYS_PROMPT = \"\"\"\n",
    "You write short, polite B2B cold emails.\n",
    "\n",
    "Your job:\n",
    "- Read the lead_profile and style_profile I give you.\n",
    "- Then write a ready-to-send cold email for that specific person.\n",
    "\n",
    "Hard rules:\n",
    "- 60â€“120 words.\n",
    "- One clear call to action.\n",
    "- No links in the first email.\n",
    "- Use ONLY information from lead_profile. Do not invent facts.\n",
    "- Use greeting and closing from style_profile.\n",
    "- DO NOT output instructions, guidelines, or placeholders.\n",
    "- DO NOT explain what you are doing.\n",
    "\n",
    "Output format (VERY IMPORTANT):\n",
    "Return ONLY a single JSON object with this exact shape:\n",
    "{\"subject\": \"<short subject line>\", \"body\": \"<full email body text>\"}\n",
    "\"\"\"\n",
    "\n",
    "# Placeholders simulate the input data structure for fine-tuning.\n",
    "placeholder_lead_profile = {\n",
    "  \"name\": \"{first_name} {last_name}\",\n",
    "  \"role\": \"{role}\",\n",
    "  \"company\": \"{company}\",\n",
    "  \"objective\": \"Write a professional email based on the Enron corpus style.\",\n",
    "}\n",
    "placeholder_style_profile = {\n",
    "  \"greeting\": \"Dear {first_name},\",\n",
    "  \"closing\": \"Sincerely,\\\\n{sender_name}\",\n",
    "  \"assertiveness\": \"medium\",\n",
    "  \"formality\": \"high\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81af81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and sampling 5000 records...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. DATA LOADING, SAMPLING, AND FORMATTING ---\n",
    "\n",
    "print(f\"Loading and sampling {SAMPLES_TO_USE} records...\")\n",
    "try:\n",
    "    df = pd.read_csv('filtered_cleaned_enron.csv')\n",
    "    sampled_df = df.sample(n=SAMPLES_TO_USE, random_state=42).copy()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'filtered_cleaned_enron.csv' not found.\")\n",
    "    exit()\n",
    "except ValueError:\n",
    "    print(f\"Error: Dataset size is less than {SAMPLES_TO_USE}. Using all available data.\")\n",
    "    sampled_df = df.copy()\n",
    "\n",
    "sampled_df['cleaned_body'] = sampled_df['cleaned_body'].astype(str)\n",
    "sampled_df['subject'] = sampled_df['subject'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908f525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready with 5000 examples.\n"
     ]
    }
   ],
   "source": [
    "def format_enron_to_mistral(row):\n",
    "    instruction = (\n",
    "      SYS_PROMPT + \"\\n\\n\" +\n",
    "      \"lead_profile = \" + json.dumps(placeholder_lead_profile, ensure_ascii=False) + \"\\n\" +\n",
    "      \"style_profile = \" + json.dumps(placeholder_style_profile, ensure_ascii=False)\n",
    "    )\n",
    "\n",
    "    # Note: Newlines are escaped for valid JSON string response\n",
    "    response_json = {\n",
    "        \"subject\": row['subject'],\n",
    "        \"body\": row['cleaned_body'].replace('\\n', '\\\\n').replace('\\r', '')\n",
    "    }\n",
    "    response = json.dumps(response_json, ensure_ascii=False)\n",
    "\n",
    "    # Apply the Mistral instruction template\n",
    "    formatted_text = f\"<s>[INST] {instruction} [/INST] {response} </s>\"\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "dataset = sampled_df.apply(format_enron_to_mistral, axis=1, result_type='expand')\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "print(f\"Dataset ready with {len(hf_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ed34d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for QLoRA fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.06s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. QLoRA and Model Configuration ---\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loading model for QLoRA fine-tuning...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d0edea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 4004.19 examples/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. OPTIMIZED TRAINING ARGUMENTS ---\n",
    "\n",
    "hf_dataset = hf_dataset.map(lambda b: tokenizer(b[\"text\"], truncation=True, max_length=768, padding=\"max_length\"), batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f6877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 455516.41 examples/s]\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 455516.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=hf_dataset,\n",
    "    peft_config=model.peft_config[list(model.peft_config.keys())[0]], # Get the LoRA config\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d34a1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [314/314 6:09:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.633100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=314, training_loss=0.7403470604283036, metrics={'train_runtime': 22244.0644, 'train_samples_per_second': 0.45, 'train_steps_per_second': 0.014, 'total_flos': 3.2959195250688e+17, 'train_loss': 0.7403470604283036, 'entropy': 0.6375446280218521, 'num_tokens': 7680000.0, 'mean_token_accuracy': 0.8600618704310004, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b930ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Model and tokenizer saved locally to mistral-7b-enron-email-finetune.\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"\\nTraining complete. Model and tokenizer saved locally to {OUTPUT_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d71dffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Sales Email with Fine-tuned Model ---\n",
      "\n",
      "Error: Failed to parse model output as JSON.\n",
      "Raw Output: {\"subject\": \"Re: Route Optimization\", \"body\": \"Hi David,\\n\\t\\tI'm interested in the Route Optimization. I'll have my team look at the demo and we'll get back to you with our thoughts.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\n",
      "Error: Failed to parse model output as JSON.\n",
      "Raw Output: {\"subject\": \"Re: Route Optimization\", \"body\": \"Hi David,\\n\\t\\tI'm interested in the Route Optimization. I'll have my team look at the demo and we'll get back to you with our thoughts.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\n"
     ]
    }
   ],
   "source": [
    "## ðŸ§ª Model Testing Script (Sales/Product Pitch Scenario)\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# --- 1. Define Test Data ---\n",
    "# Scenario: Pitching a new software product to a potential client\n",
    "test_lead_profile = {\n",
    "  \"name\": \"Michael Chen\",\n",
    "  \"role\": \"Head of Operations\",\n",
    "  \"company\": \"LogisticsFast\",\n",
    "  # The objective here defines the sales goal\n",
    "  \"objective\": \"Propose a demo of our 'RouteOptimizer' software to help reduce their delivery times by 20%.\",\n",
    "}\n",
    "\n",
    "test_style_profile = {\n",
    "  \"greeting\": \"Hi Michael,\",\n",
    "  \"closing\": \"Cheers,\\nDavid\",\n",
    "  \"assertiveness\": \"high\",  # Higher assertiveness for a sales pitch\n",
    "  \"formality\": \"medium\"\n",
    "}\n",
    "\n",
    "# --- 2. Format Input for Model ---\n",
    "if 'SYS_PROMPT' not in locals():\n",
    "    SYS_PROMPT = \"\"\"\n",
    "You write short, polite B2B cold emails.\n",
    "\n",
    "Your job:\n",
    "- Read the lead_profile and style_profile I give you.\n",
    "- Then write a ready-to-send cold email for that specific person.\n",
    "\n",
    "Hard rules:\n",
    "- Email should be less that 120 words.\n",
    "- One clear call to action.\n",
    "- No links in the first email.\n",
    "- Use ONLY information from lead_profile. Do not invent facts.\n",
    "- Use greeting and closing from style_profile.\n",
    "- DO NOT output instructions, guidelines, or placeholders.\n",
    "- DO NOT explain what you are doing.\n",
    "\n",
    "Output format (VERY IMPORTANT):\n",
    "Return ONLY a single JSON object with this exact shape:\n",
    "{\"subject\": \"<short subject line>\", \"body\": \"<full email body text>\"}\n",
    "\"\"\"\n",
    "\n",
    "instruction = (\n",
    "  SYS_PROMPT + \"\\n\\n\" +\n",
    "  \"lead_profile = \" + json.dumps(test_lead_profile, ensure_ascii=False) + \"\\n\" +\n",
    "  \"style_profile = \" + json.dumps(test_style_profile, ensure_ascii=False)\n",
    ")\n",
    "\n",
    "formatted_input = f\"<s>[INST] {instruction} [/INST]\"\n",
    "\n",
    "# --- 3. Generate Email ---\n",
    "print(\"\\n--- Generating Sales Email with Fine-tuned Model ---\\n\")\n",
    "\n",
    "model_input = tokenizer(formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# --- FIX: Cast lm_head to float32 ---\n",
    "# Ensures weights match the input tensor type to prevent RuntimeError\n",
    "if hasattr(model, \"base_model\") and hasattr(model.base_model, \"lm_head\"):\n",
    "    model.base_model.lm_head.to(torch.float32)\n",
    "elif hasattr(model, \"lm_head\"):\n",
    "    model.lm_head.to(torch.float32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **model_input,\n",
    "        max_new_tokens=600,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "full_output = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\n",
    "# --- 4. Post-process and Print ---\n",
    "try:\n",
    "    response_start_index = full_output.rfind('[/INST]')\n",
    "    if response_start_index != -1:\n",
    "        raw_json_output = full_output[response_start_index + len('[/INST]'):].strip()\n",
    "        \n",
    "        if raw_json_output.endswith('</s>'):\n",
    "            raw_json_output = raw_json_output[:-4].strip()\n",
    "\n",
    "        try:\n",
    "            email_data = json.loads(raw_json_output)\n",
    "            print(\"Successfully Parsed JSON Output:\")\n",
    "            print(\"Subject:\", email_data.get('subject', 'N/A'))\n",
    "            print(\"---\")\n",
    "            body_text = email_data.get('body', 'N/A').replace('\\\\n', '\\n')\n",
    "            print(body_text)\n",
    "            print(\"---\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Failed to parse model output as JSON.\")\n",
    "            print(\"Raw Output:\", raw_json_output)\n",
    "    else:\n",
    "        print(\"Error: Could not find the instruction closing tag '[/INST]' in the output.\")\n",
    "        print(\"Full Model Output:\", full_output)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during processing: {e}\")\n",
    "    print(\"Full Model Output:\", full_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
